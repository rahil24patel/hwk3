{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ee9329-9fde-4d74-a61c-1af48e70618e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HWK3_ROOT: /home/rpat638/econ470/a0/work/hwk3\n",
      "YEARS: [2010, 2011, 2012, 2013, 2014, 2015]\n",
      "PROCESSED_DIR: /home/rpat638/econ470/a0/work/hwk3/data/processed\n",
      "CACHE_DIR: /home/rpat638/econ470/a0/work/hwk3/data/cache\n",
      "FORCE_REBUILD_ENROLL: False\n",
      "FORCE_REBUILD_LAND: False\n",
      "FORCE_REBUILD_STARS: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import json\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 220)\n",
    "pd.set_option(\"display.width\", 180)\n",
    "\n",
    "YEARS = list(range(2010, 2016))\n",
    "\n",
    "HWK3_ROOT = Path.cwd()\n",
    "CODE_DIR = HWK3_ROOT / \"code\"\n",
    "CACHE_DIR = HWK3_ROOT / \"data\" / \"cache\"\n",
    "PROCESSED_DIR = HWK3_ROOT / \"data\" / \"processed\"\n",
    "\n",
    "CODE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Rebuild switches\n",
    "FORCE_REBUILD_ENROLL = False\n",
    "FORCE_REBUILD_LAND   = False\n",
    "FORCE_REBUILD_STARS  = True\n",
    "\n",
    "OUT_PLAN_COUNTY_YEAR = PROCESSED_DIR / \"hw3_plan_county_year_2010_2015.csv\"\n",
    "OUT_PLAN_YEAR        = PROCESSED_DIR / \"hw3_plan_year_2010_2015.csv\"\n",
    "OUT_CONTRACT_RATINGS = PROCESSED_DIR / \"hw3_contract_ratings_2010_2015.csv\"\n",
    "OUT_RATING_DIST      = PROCESSED_DIR / \"hw3_rating_distribution_2010_2015.csv\"\n",
    "OUT_RD_2010          = PROCESSED_DIR / \"hw3_rd_2010_threshold_samples.csv\"\n",
    "OUT_MANIFEST         = PROCESSED_DIR / \"hw3_build_manifest.json\"\n",
    "\n",
    "print(\"HWK3_ROOT:\", HWK3_ROOT)\n",
    "print(\"YEARS:\", YEARS)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
    "print(\"CACHE_DIR:\", CACHE_DIR)\n",
    "print(\"FORCE_REBUILD_ENROLL:\", FORCE_REBUILD_ENROLL)\n",
    "print(\"FORCE_REBUILD_LAND:\", FORCE_REBUILD_LAND)\n",
    "print(\"FORCE_REBUILD_STARS:\", FORCE_REBUILD_STARS)\n",
    "\n",
    "manifest: dict = {\"ma_root\": None, \"files_used\": {}, \"outputs\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33ad2767-1d56-48d5-b53f-79703f4d9753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA_ROOT: /scion/5261/econ470001/ma-data/ma\n",
      "ENROLL_DIR exists: True /scion/5261/econ470001/ma-data/ma/enrollment\n",
      "LANDSCAPE_DIR exists: True /scion/5261/econ470001/ma-data/ma/landscape\n",
      "STAR_DIR exists: True /scion/5261/econ470001/ma-data/ma/star-ratings\n"
     ]
    }
   ],
   "source": [
    "def pick_existing(paths: list[Path]) -> Path:\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not find MA_ROOT. Tried:\\n\" + \"\\n\".join(map(str, paths)))\n",
    "\n",
    "CANDIDATE_MA_ROOTS = [\n",
    "    Path(\"/scion/5261/econ470001/ma-data/ma\"),\n",
    "    HWK3_ROOT.parent / \"ma-data\" / \"ma\",\n",
    "    HWK3_ROOT.parent.parent / \"ma-data\" / \"ma\",\n",
    "]\n",
    "\n",
    "MA_ROOT = pick_existing(CANDIDATE_MA_ROOTS)\n",
    "\n",
    "ENROLL_DIR    = MA_ROOT / \"enrollment\"\n",
    "LANDSCAPE_DIR = MA_ROOT / \"landscape\"\n",
    "STAR_DIR      = MA_ROOT / \"star-ratings\"\n",
    "\n",
    "manifest[\"ma_root\"] = str(MA_ROOT)\n",
    "\n",
    "print(\"MA_ROOT:\", MA_ROOT)\n",
    "print(\"ENROLL_DIR exists:\", ENROLL_DIR.exists(), ENROLL_DIR)\n",
    "print(\"LANDSCAPE_DIR exists:\", LANDSCAPE_DIR.exists(), LANDSCAPE_DIR)\n",
    "print(\"STAR_DIR exists:\", STAR_DIR.exists(), STAR_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d079cfa5-26f4-40ef-9d74-176b6faa5f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_colname(s: str) -> str:\n",
    "    s = str(s).strip().lower()\n",
    "    s = re.sub(r\"[^a-z0-9]+\", \"_\", s)\n",
    "    s = re.sub(r\"_+\", \"_\", s).strip(\"_\")\n",
    "    return s\n",
    "\n",
    "def _make_unique(cols: list[str]) -> list[str]:\n",
    "    seen = {}\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        if c not in seen:\n",
    "            seen[c] = 0\n",
    "            out.append(c)\n",
    "        else:\n",
    "            seen[c] += 1\n",
    "            out.append(f\"{c}_dup{seen[c]}\")\n",
    "    return out\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    cols = [norm_colname(c) for c in df.columns]\n",
    "    df.columns = _make_unique(cols)\n",
    "    return df\n",
    "\n",
    "def to_num(x):\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        if x.shape[1] == 0:\n",
    "            return pd.Series([np.nan] * len(x), index=x.index)\n",
    "        x = x.iloc[:, 0]\n",
    "    return pd.to_numeric(x, errors=\"coerce\")\n",
    "\n",
    "def to_num_clean(x):\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        if x.shape[1] == 0:\n",
    "            return pd.Series([np.nan] * len(x), index=x.index)\n",
    "        x = x.iloc[:, 0]\n",
    "    s = x.astype(str)\n",
    "    s = s.str.replace(r\"[,%$]\", \"\", regex=True)\n",
    "    s = s.str.replace(r\"[^0-9\\.\\-]\", \"\", regex=True)\n",
    "    return pd.to_numeric(s, errors=\"coerce\")\n",
    "\n",
    "def clean_contract_id(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip().str.upper()\n",
    "    s = s.str.replace(r\"[^A-Z0-9]\", \"\", regex=True)\n",
    "    s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n",
    "    return s\n",
    "\n",
    "def clean_plan_id(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan})\n",
    "    s = s.apply(lambda v: v.zfill(3) if isinstance(v, str) else v)\n",
    "    s = s.where(s.str.len() == 3, np.nan)\n",
    "    return s\n",
    "\n",
    "def coerce_fips(s: pd.Series) -> pd.Series:\n",
    "    s = s.astype(str).str.strip()\n",
    "    s = s.str.replace(r\"[^0-9]\", \"\", regex=True)\n",
    "    s = s.replace({\"\": np.nan, \"nan\": np.nan, \"NaN\": np.nan})\n",
    "    s = s.apply(lambda v: v.zfill(5) if isinstance(v, str) else v)\n",
    "    s = s.where(s.str.len() == 5, np.nan)\n",
    "    return s\n",
    "\n",
    "def safe_rglob(root: Path, pattern: str, limit: int = 200000) -> list[Path]:\n",
    "    if not root.exists():\n",
    "        return []\n",
    "    out: list[Path] = []\n",
    "    try:\n",
    "        for p in root.rglob(pattern):\n",
    "            out.append(p)\n",
    "            if len(out) >= limit:\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print(\"rglob failed:\", root, pattern, \"err:\", e)\n",
    "    return sorted(out)\n",
    "\n",
    "def parse_year_month_from_name(name: str) -> tuple[int | None, int | None]:\n",
    "    n = name.lower()\n",
    "    pats = [\n",
    "        r\"(?P<year>20\\d{2})[_-](?P<month>0[1-9]|1[0-2])(?=[^0-9]|$)\",\n",
    "        r\"(?P<year>20\\d{2})(?P<month>0[1-9]|1[0-2])(?=[^0-9]|$)\",\n",
    "        r\"(?P<month>0[1-9]|1[0-2])[_-](?P<year>20\\d{2})(?=[^0-9]|$)\",\n",
    "    ]\n",
    "    for pat in pats:\n",
    "        m = re.search(pat, n)\n",
    "        if m:\n",
    "            return int(m.group(\"year\")), int(m.group(\"month\"))\n",
    "    return None, None\n",
    "\n",
    "def pick_col(cols: list[str], must: list[str], prefer: list[str] | None = None, avoid: list[str] | None = None) -> str | None:\n",
    "    prefer = prefer or []\n",
    "    avoid = avoid or []\n",
    "    best, best_score = None, -10**9\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if any(a in cl for a in avoid):\n",
    "            continue\n",
    "        if not all(m in cl for m in must):\n",
    "            continue\n",
    "        score = 0\n",
    "        for p in prefer:\n",
    "            if p in cl:\n",
    "                score += 2\n",
    "        for m in must:\n",
    "            if m in cl:\n",
    "                score += 1\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = c\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1db5673-5c5a-42e2-ba6f-22caee082361",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNKSIZE = 500_000\n",
    "\n",
    "def _detect_delim_from_bytes(b: bytes) -> str:\n",
    "    s = b.decode(\"latin1\", errors=\"ignore\")\n",
    "    counts = {\",\": s.count(\",\"), \"\\t\": s.count(\"\\t\"), \"|\": s.count(\"|\"), \";\": s.count(\";\")}\n",
    "    return max(counts, key=counts.get)\n",
    "\n",
    "def safe_read_csv(path: Path, *, skiprows: int = 0, header: int | None = 0, nrows: int | None = None, usecols=None) -> pd.DataFrame:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b = f.read(12000)\n",
    "    sep = _detect_delim_from_bytes(b)\n",
    "\n",
    "    for enc in [\"utf-8\", \"cp1252\", \"latin1\"]:\n",
    "        try:\n",
    "            return pd.read_csv(\n",
    "                path,\n",
    "                dtype=str,\n",
    "                skiprows=skiprows,\n",
    "                header=header,\n",
    "                sep=sep,\n",
    "                engine=\"c\",\n",
    "                encoding=enc,\n",
    "                encoding_errors=\"replace\",\n",
    "                nrows=nrows,\n",
    "                usecols=usecols,\n",
    "            )\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    return pd.read_csv(\n",
    "        path,\n",
    "        dtype=str,\n",
    "        skiprows=skiprows,\n",
    "        header=header,\n",
    "        sep=sep,\n",
    "        engine=\"python\",\n",
    "        encoding=\"latin1\",\n",
    "        encoding_errors=\"replace\",\n",
    "        on_bad_lines=\"skip\",\n",
    "        nrows=nrows,\n",
    "        usecols=usecols,\n",
    "    )\n",
    "\n",
    "def detect_header_row_csv(path: Path, nrows_scan: int = 60) -> int:\n",
    "    try:\n",
    "        raw = safe_read_csv(path, skiprows=0, header=None, nrows=nrows_scan)\n",
    "    except Exception:\n",
    "        return 0\n",
    "\n",
    "    keys = [\"contract\", \"plan\", \"pbp\", \"enroll\", \"enrollment\", \"fips\", \"county\", \"star\", \"rating\"]\n",
    "    bad  = [\"dictionary\", \"layout\", \"readme\"]\n",
    "\n",
    "    best_i, best_score = 0, -10**9\n",
    "    for i in range(min(len(raw), nrows_scan)):\n",
    "        row = raw.iloc[i].astype(str).str.lower()\n",
    "        score = 0\n",
    "        for k in keys:\n",
    "            score += row.str.contains(k, na=False).sum()\n",
    "        for b in bad:\n",
    "            score -= 10 * row.str.contains(b, na=False).sum()\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_i = i\n",
    "\n",
    "    return int(best_i)\n",
    "\n",
    "def read_csv_autoheader(path: Path, nrows_scan: int = 60) -> pd.DataFrame:\n",
    "    h = detect_header_row_csv(path, nrows_scan=nrows_scan)\n",
    "    if h == 0:\n",
    "        return safe_read_csv(path, skiprows=0, header=0)\n",
    "    return safe_read_csv(path, skiprows=h, header=0)\n",
    "\n",
    "def read_any(path: Path, nrows_scan: int = 60) -> pd.DataFrame:\n",
    "    if path.suffix.lower() == \".csv\":\n",
    "        return read_csv_autoheader(path, nrows_scan=nrows_scan)\n",
    "    if path.suffix.lower() == \".parquet\":\n",
    "        return pd.read_parquet(path)\n",
    "    raise ValueError(\"Unsupported: \" + str(path))\n",
    "\n",
    "def read_first_csv_inside_zip(zip_path: Path) -> pd.DataFrame:\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        names = [n for n in z.namelist() if n.lower().endswith(\".csv\")]\n",
    "        if not names:\n",
    "            raise FileNotFoundError(\"No CSV inside zip: \" + str(zip_path))\n",
    "        names_sorted = sorted(names, key=lambda s: ((\"contract\" not in s.lower()), (\"star\" not in s.lower()), len(s)))\n",
    "        target = names_sorted[0]\n",
    "        with z.open(target) as f:\n",
    "            return pd.read_csv(f, dtype=str, encoding_errors=\"replace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8255e83f-9411-48f1-bbce-d0a51bfe1447",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010 months found: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "2010 month 1 pick: /scion/5261/econ470001/ma-data/ma/enrollment/Extracted Data/CPSC_Enrollment_Info_2010_01.csv\n"
     ]
    }
   ],
   "source": [
    "def _score_enroll_candidate(path: Path) -> int:\n",
    "    nm = path.name.lower()\n",
    "    if \"contract_info\" in nm or \"dictionary\" in nm or \"layout\" in nm or \"readme\" in nm:\n",
    "        return -10**9\n",
    "\n",
    "    score = 0\n",
    "    if \"enroll\" in nm or \"enrollment\" in nm:\n",
    "        score += 50\n",
    "    if \"monthly\" in nm:\n",
    "        score += 5\n",
    "\n",
    "    try:\n",
    "        raw = safe_read_csv(path, skiprows=0, header=None, nrows=35)\n",
    "    except Exception:\n",
    "        return -10**9\n",
    "\n",
    "    best = -10**9\n",
    "    for i in range(min(len(raw), 35)):\n",
    "        row = raw.iloc[i].astype(str).str.lower()\n",
    "        s = 0\n",
    "        s += 4 * row.str.contains(\"contract\", na=False).sum()\n",
    "        s += 4 * row.str.contains(\"plan\", na=False).sum()\n",
    "        s += 8 * row.str.contains(\"enroll\", na=False).sum()\n",
    "        s += 2 * row.str.contains(\"pbp\", na=False).sum()\n",
    "        s += 2 * row.str.contains(\"fips\", na=False).sum()\n",
    "        if row.str.contains(\"info\", na=False).sum() > 0:\n",
    "            s -= 30\n",
    "        best = max(best, s)\n",
    "\n",
    "    if best < 10:\n",
    "        return -10**9\n",
    "\n",
    "    return int(score + best)\n",
    "\n",
    "def find_monthly_enrollment_files(year: int) -> dict[int, Path]:\n",
    "    files = safe_rglob(ENROLL_DIR, \"*.csv\", limit=200000) + safe_rglob(ENROLL_DIR, \"*.parquet\", limit=200000)\n",
    "\n",
    "    buckets: dict[int, list[Path]] = {m: [] for m in range(1, 13)}\n",
    "    for p in files:\n",
    "        y, mo = parse_year_month_from_name(p.name)\n",
    "        if y == year and mo is not None and 1 <= mo <= 12:\n",
    "            buckets[mo].append(p)\n",
    "\n",
    "    chosen: dict[int, Path] = {}\n",
    "    for mo, hits in buckets.items():\n",
    "        if not hits:\n",
    "            continue\n",
    "        scored = [(_score_enroll_candidate(p), p) for p in hits]\n",
    "        scored.sort(key=lambda t: t[0], reverse=True)\n",
    "        best_score, best_path = scored[0]\n",
    "        if best_score <= -10**8:\n",
    "            best_path = sorted(hits, key=lambda q: (len(q.name), str(q)))[0]\n",
    "        chosen[mo] = best_path\n",
    "\n",
    "    return chosen\n",
    "\n",
    "picked_2010 = find_monthly_enrollment_files(2010)\n",
    "print(\"2010 months found:\", sorted(picked_2010.keys()))\n",
    "print(\"2010 month 1 pick:\", picked_2010.get(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66af7e71-dacc-4660-9342-23f0117497b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Picked file: CPSC_Enrollment_Info_2010_01.csv\n",
      "Raw columns: ['Contract Number', 'Plan ID', 'SSA State County Code', 'FIPS State County Code', 'State', 'County', 'Enrollment']\n",
      "Normalized: ['contract_number', 'plan_id', 'ssa_state_county_code', 'fips_state_county_code', 'state', 'county', 'enrollment']\n"
     ]
    }
   ],
   "source": [
    "def _detect_sep_and_encoding(path: Path) -> tuple[str, str]:\n",
    "    with open(path, \"rb\") as f:\n",
    "        b = f.read(12000)\n",
    "    sep = _detect_delim_from_bytes(b)\n",
    "\n",
    "    for enc in [\"utf-8\", \"cp1252\", \"latin1\"]:\n",
    "        try:\n",
    "            _ = pd.read_csv(path, dtype=str, sep=sep, engine=\"c\", encoding=enc, encoding_errors=\"replace\", nrows=5)\n",
    "            return sep, enc\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    return sep, \"latin1\"\n",
    "\n",
    "def _find_enrollment_cols_norm(norm_cols: list[str]) -> tuple[str, str, str, str]:\n",
    "    c_contract = pick_col(norm_cols, must=[\"contract\"], prefer=[\"id\", \"number\"])\n",
    "    c_plan = pick_col(norm_cols, must=[\"plan\"], prefer=[\"id\"], avoid=[\"name\"])\n",
    "    if c_plan is None:\n",
    "        c_plan = pick_col(norm_cols, must=[\"pbp\"], prefer=[], avoid=[\"name\"])\n",
    "\n",
    "    c_enroll = pick_col(norm_cols, must=[\"enroll\"], prefer=[], avoid=[\"eligible\", \"eligib\"])\n",
    "    if c_enroll is None:\n",
    "        c_enroll = pick_col(norm_cols, must=[\"enrollment\"], prefer=[], avoid=[\"eligible\", \"eligib\"])\n",
    "\n",
    "    c_fips = None\n",
    "    for cand in [\"fips_state_county_code\", \"fips\", \"county_fips\"]:\n",
    "        if cand in norm_cols:\n",
    "            c_fips = cand\n",
    "            break\n",
    "    if c_fips is None:\n",
    "        c_fips = pick_col(norm_cols, must=[\"fips\"], prefer=[], avoid=[])\n",
    "\n",
    "    if c_fips is None:\n",
    "        c_fips = \"fips_missing\"\n",
    "\n",
    "    return c_contract, c_plan, c_enroll, c_fips\n",
    "\n",
    "def load_enrollment_month(path: Path) -> pd.DataFrame:\n",
    "    h = detect_header_row_csv(path, nrows_scan=80)\n",
    "    skip = h if h > 0 else 0\n",
    "    sep, enc = _detect_sep_and_encoding(path)\n",
    "\n",
    "    head = pd.read_csv(\n",
    "        path, dtype=str, sep=sep, engine=\"c\", encoding=enc, encoding_errors=\"replace\",\n",
    "        skiprows=skip, header=0, nrows=5\n",
    "    )\n",
    "\n",
    "    orig_cols = list(head.columns)\n",
    "    norm_cols = [norm_colname(c) for c in orig_cols]\n",
    "\n",
    "    norm_to_idx = {}\n",
    "    for i, nc in enumerate(norm_cols):\n",
    "        if nc not in norm_to_idx:\n",
    "            norm_to_idx[nc] = i\n",
    "\n",
    "    c_contract, c_plan, c_enroll, c_fips = _find_enrollment_cols_norm(norm_cols)\n",
    "\n",
    "    if c_contract is None or c_plan is None or c_enroll is None:\n",
    "        raise KeyError(f\"Enrollment file missing columns. file={path} norm_cols_sample={norm_cols[:80]}\")\n",
    "\n",
    "    idxs = [norm_to_idx[c_contract], norm_to_idx[c_plan], norm_to_idx[c_enroll]]\n",
    "    has_fips = (c_fips in norm_to_idx)\n",
    "    if has_fips:\n",
    "        idxs.append(norm_to_idx[c_fips])\n",
    "\n",
    "    agg: dict[tuple[str, str, str], float] = {}\n",
    "\n",
    "    reader = pd.read_csv(\n",
    "        path,\n",
    "        dtype=str,\n",
    "        sep=sep,\n",
    "        engine=\"c\",\n",
    "        encoding=enc,\n",
    "        encoding_errors=\"replace\",\n",
    "        skiprows=skip,\n",
    "        header=0,\n",
    "        usecols=idxs,\n",
    "        chunksize=CHUNKSIZE,\n",
    "    )\n",
    "\n",
    "    for chunk in reader:\n",
    "        chunk = normalize_columns(chunk)\n",
    "\n",
    "        d = pd.DataFrame()\n",
    "        d[\"contractid\"] = clean_contract_id(chunk[c_contract])\n",
    "        d[\"planid\"] = clean_plan_id(chunk[c_plan])\n",
    "        d[\"enrollment\"] = to_num(chunk[c_enroll])\n",
    "\n",
    "        if has_fips:\n",
    "            d[\"fips\"] = coerce_fips(chunk[c_fips])\n",
    "        else:\n",
    "            d[\"fips\"] = np.nan\n",
    "\n",
    "        d = d.dropna(subset=[\"contractid\", \"planid\", \"fips\"])\n",
    "        d = d[d[\"fips\"] != \"00000\"]\n",
    "\n",
    "        g = d.groupby([\"contractid\", \"planid\", \"fips\"], as_index=False)[\"enrollment\"].sum()\n",
    "\n",
    "        for row in g.itertuples(index=False):\n",
    "            key = (row.contractid, row.planid, row.fips)\n",
    "            agg[key] = agg.get(key, 0.0) + (0.0 if pd.isna(row.enrollment) else float(row.enrollment))\n",
    "\n",
    "    out = pd.DataFrame([(k[0], k[1], k[2], v) for k, v in agg.items()],\n",
    "                       columns=[\"contractid\", \"planid\", \"fips\", \"enrollment\"])\n",
    "    return out\n",
    "\n",
    "p = picked_2010[1]\n",
    "h = detect_header_row_csv(p, nrows_scan=80)\n",
    "sep, enc = _detect_sep_and_encoding(p)\n",
    "head = pd.read_csv(p, dtype=str, sep=sep, engine=\"c\", encoding=enc, encoding_errors=\"replace\", skiprows=h, header=0, nrows=3)\n",
    "print(\"Picked file:\", p.name)\n",
    "print(\"Raw columns:\", list(head.columns))\n",
    "print(\"Normalized:\", [norm_colname(c) for c in head.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d579326-36ae-4ceb-a06d-64a0e089882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_plan_county_year_enrollment(year: int) -> tuple[pd.DataFrame, dict]:\n",
    "    files_by_month = find_monthly_enrollment_files(year)\n",
    "    months = sorted(files_by_month.keys())\n",
    "\n",
    "    meta = {\n",
    "        \"year\": year,\n",
    "        \"months_found\": months,\n",
    "        \"month_files\": {m: str(files_by_month[m]) for m in months},\n",
    "    }\n",
    "\n",
    "    if not months:\n",
    "        raise FileNotFoundError(f\"No monthly enrollment files found for {year} under {ENROLL_DIR}\")\n",
    "\n",
    "    parts = []\n",
    "    for m in months:\n",
    "        p = files_by_month[m]\n",
    "        d = load_enrollment_month(p)\n",
    "        d[\"month\"] = m\n",
    "        parts.append(d)\n",
    "\n",
    "    allm = pd.concat(parts, ignore_index=True)\n",
    "\n",
    "    g = (\n",
    "        allm.groupby([\"contractid\", \"planid\", \"fips\"], as_index=False)\n",
    "           .agg(enroll_sum=(\"enrollment\", \"sum\"),\n",
    "                months_observed=(\"month\", \"nunique\"))\n",
    "    )\n",
    "    g[\"avg_enrollment\"] = g[\"enroll_sum\"] / g[\"months_observed\"].replace(0, np.nan)\n",
    "    g[\"year\"] = year\n",
    "\n",
    "    g = g[[\"year\", \"contractid\", \"planid\", \"fips\", \"months_observed\", \"avg_enrollment\"]].copy()\n",
    "    return g, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6110bc9f-2cd4-4dec-98f0-871d88779ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached: /home/rpat638/econ470/a0/work/hwk3/data/cache/hw3_plan_county_year_enrollment_2010_2015.csv rows: 14090286\n",
      "   year contractid planid   fips  months_observed  avg_enrollment\n",
      "0  2010      E0654    801  01001               12             0.0\n",
      "1  2010      E0654    801  01003               12             0.0\n",
      "2  2010      E0654    801  01005               12             0.0\n",
      "3  2010      E0654    801  01007               12             0.0\n",
      "4  2010      E0654    801  01009               12             0.0\n",
      "Rows: 14090286 | Years: [2010, 2011, 2012, 2013, 2014, 2015]\n"
     ]
    }
   ],
   "source": [
    "CACHE_ENROLL = CACHE_DIR / \"hw3_plan_county_year_enrollment_2010_2015.csv\"\n",
    "CACHE_META   = CACHE_DIR / \"hw3_enrollment_file_manifest.json\"\n",
    "\n",
    "if FORCE_REBUILD_ENROLL:\n",
    "    for p in [CACHE_ENROLL, CACHE_META]:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "            print(\"Deleted:\", p)\n",
    "\n",
    "if CACHE_ENROLL.exists():\n",
    "    enroll_cty = pd.read_csv(CACHE_ENROLL, dtype={\"contractid\": str, \"planid\": str, \"fips\": str})\n",
    "    meta_enroll = json.loads(CACHE_META.read_text()) if CACHE_META.exists() else {}\n",
    "    print(\"Loaded cached:\", CACHE_ENROLL, \"rows:\", enroll_cty.shape[0])\n",
    "else:\n",
    "    parts = []\n",
    "    meta_enroll = {}\n",
    "    for y in YEARS:\n",
    "        print(\"Building enrollment for year:\", y)\n",
    "        d, meta = build_plan_county_year_enrollment(y)\n",
    "        parts.append(d)\n",
    "        meta_enroll[str(y)] = meta\n",
    "    enroll_cty = pd.concat(parts, ignore_index=True)\n",
    "    enroll_cty.to_csv(CACHE_ENROLL, index=False)\n",
    "    CACHE_META.write_text(json.dumps(meta_enroll, indent=2))\n",
    "    print(\"Wrote cache:\", CACHE_ENROLL)\n",
    "\n",
    "manifest[\"files_used\"][\"enrollment\"] = meta_enroll\n",
    "\n",
    "print(enroll_cty.head())\n",
    "print(\"Rows:\", enroll_cty.shape[0], \"| Years:\", sorted(enroll_cty[\"year\"].unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c59e25b6-3fd3-4dcc-91fe-09cf3a3e98c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean county share-sum by year (should be ~1):\n",
      "year\n",
      "2010    0.993812\n",
      "2011    0.995668\n",
      "2012    0.995359\n",
      "2013    0.994740\n",
      "2014    0.994431\n",
      "2015    0.992583\n",
      "Name: mkt_share, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "enroll_cty = enroll_cty.copy()\n",
    "enroll_cty[\"avg_enrollment\"] = to_num(enroll_cty[\"avg_enrollment\"])\n",
    "\n",
    "county_totals = (\n",
    "    enroll_cty.groupby([\"year\", \"fips\"], as_index=False)[\"avg_enrollment\"]\n",
    "              .sum()\n",
    "              .rename(columns={\"avg_enrollment\": \"avg_enrolled\"})\n",
    ")\n",
    "\n",
    "plan_county = enroll_cty.merge(county_totals, on=[\"year\", \"fips\"], how=\"left\", validate=\"m:1\")\n",
    "plan_county[\"mkt_share\"] = plan_county[\"avg_enrollment\"] / plan_county[\"avg_enrolled\"].replace(0, np.nan)\n",
    "\n",
    "chk = plan_county.groupby([\"year\", \"fips\"])[\"mkt_share\"].sum().reset_index()\n",
    "print(\"Mean county share-sum by year (should be ~1):\")\n",
    "print(chk.groupby(\"year\")[\"mkt_share\"].mean().round(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e1d731d-e12b-4173-88af-321a442f35e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded cached landscape: /home/rpat638/econ470/a0/work/hwk3/data/cache/hw3_landscape_plan_year_2010_2015.csv rows: 6399\n",
      "   year contractid planid  plan_type  partd  hmo\n",
      "0   NaN      H0104    004  Local PPO    NaN  0.0\n",
      "1   NaN      H0104    008  Local PPO    NaN  0.0\n",
      "2   NaN      H0104    002  Local PPO    NaN  0.0\n",
      "3   NaN      H2762    015     PFFS *    NaN  0.0\n",
      "4   NaN      H2762    019     PFFS *    NaN  0.0\n",
      "Rows: 6399 | Years: []\n"
     ]
    }
   ],
   "source": [
    "def _score_landscape_candidate(path: Path) -> int:\n",
    "    nm = path.name.lower()\n",
    "    if any(bad in nm for bad in [\"dictionary\", \"layout\", \"readme\"]):\n",
    "        return -10**9\n",
    "    score = 0\n",
    "    if \"land\" in nm or \"landscape\" in nm:\n",
    "        score += 10\n",
    "\n",
    "    try:\n",
    "        dfh = safe_read_csv(path, skiprows=detect_header_row_csv(path, nrows_scan=120), header=0, nrows=40)\n",
    "        dfh = normalize_columns(dfh)\n",
    "        cols = list(dfh.columns)\n",
    "    except Exception:\n",
    "        return -10**9\n",
    "\n",
    "    score += 20 if any(\"contract\" in c for c in cols) else 0\n",
    "    score += 20 if any((\"plan\" in c or \"pbp\" in c) for c in cols) else 0\n",
    "    score += 5 if any(\"type\" in c for c in cols) else 0\n",
    "    score += 5 if any(\"partd\" in c for c in cols) else 0\n",
    "\n",
    "    return score\n",
    "\n",
    "def find_landscape_file_for_year(year: int) -> Path:\n",
    "    files = (\n",
    "        safe_rglob(LANDSCAPE_DIR, \"*.csv\", limit=200000)\n",
    "        + safe_rglob(LANDSCAPE_DIR, \"*.parquet\", limit=200000)\n",
    "        + safe_rglob(LANDSCAPE_DIR, \"*.zip\", limit=200000)\n",
    "    )\n",
    "    y = str(year)\n",
    "    cand = [p for p in files if y in str(p)]\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(f\"No landscape candidates found for year {year} under {LANDSCAPE_DIR}\")\n",
    "    scored = [(_score_landscape_candidate(p), p) for p in cand]\n",
    "    scored.sort(key=lambda t: t[0], reverse=True)\n",
    "    best_score, best_path = scored[0]\n",
    "    if best_score <= -10**8:\n",
    "        best_path = sorted(cand, key=lambda q: (len(q.name), str(q)))[0]\n",
    "    return best_path\n",
    "\n",
    "def load_landscape_year(year: int) -> tuple[pd.DataFrame, dict]:\n",
    "    p = find_landscape_file_for_year(year)\n",
    "    df = read_first_csv_inside_zip(p) if p.suffix.lower() == \".zip\" else read_any(p, nrows_scan=120)\n",
    "    df = normalize_columns(df)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    c_contract = pick_col(cols, must=[\"contract\"], prefer=[\"id\", \"number\"])\n",
    "    c_plan = pick_col(cols, must=[\"plan\"], prefer=[\"id\"], avoid=[\"name\"])\n",
    "    if c_plan is None:\n",
    "        c_plan = pick_col(cols, must=[\"pbp\"], prefer=[], avoid=[\"name\"])\n",
    "\n",
    "    c_plan_type = pick_col(cols, must=[\"type\"], prefer=[\"plan\"], avoid=[])\n",
    "    if c_plan_type is None:\n",
    "        c_plan_type = pick_col(cols, must=[\"plan\"], prefer=[\"type\"], avoid=[\"name\"])\n",
    "\n",
    "    c_partd = None\n",
    "    for cand in [\"partd\", \"part_d\", \"partd_yn\", \"part_d_yn\"]:\n",
    "        if cand in cols:\n",
    "            c_partd = cand\n",
    "            break\n",
    "\n",
    "    if c_contract is None or c_plan is None:\n",
    "        raise KeyError(f\"Landscape missing contract/plan. year={year} file={p} cols_sample={cols[:80]}\")\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"year\"] = int(year)\n",
    "    out[\"contractid\"] = clean_contract_id(df[c_contract])\n",
    "    out[\"planid\"] = clean_plan_id(df[c_plan])\n",
    "    out[\"plan_type\"] = df[c_plan_type].astype(str) if c_plan_type and c_plan_type in df.columns else \"\"\n",
    "    out[\"partd\"] = to_num_clean(df[c_partd]) if c_partd and c_partd in df.columns else np.nan\n",
    "\n",
    "    pt = out[\"plan_type\"].astype(str).str.upper()\n",
    "    out[\"hmo\"] = np.where(pt.str.contains(\"HMO\", na=False), 1.0, 0.0)\n",
    "\n",
    "    out = out.dropna(subset=[\"contractid\", \"planid\"])\n",
    "    out = out.drop_duplicates(subset=[\"year\", \"contractid\", \"planid\"])\n",
    "    return out, {\"file\": str(p)}\n",
    "\n",
    "CACHE_LAND = CACHE_DIR / \"hw3_landscape_plan_year_2010_2015.csv\"\n",
    "CACHE_LAND_META = CACHE_DIR / \"hw3_landscape_file_manifest.json\"\n",
    "\n",
    "if FORCE_REBUILD_LAND:\n",
    "    for p in [CACHE_LAND, CACHE_LAND_META]:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "            print(\"Deleted:\", p)\n",
    "\n",
    "if CACHE_LAND.exists():\n",
    "    land = pd.read_csv(CACHE_LAND, dtype={\"contractid\": str, \"planid\": str})\n",
    "    land_meta = json.loads(CACHE_LAND_META.read_text()) if CACHE_LAND_META.exists() else {}\n",
    "    print(\"Loaded cached landscape:\", CACHE_LAND, \"rows:\", land.shape[0])\n",
    "else:\n",
    "    parts = []\n",
    "    land_meta = {}\n",
    "    for y in YEARS:\n",
    "        print(\"Loading landscape for year:\", y)\n",
    "        d, meta = load_landscape_year(y)\n",
    "        parts.append(d)\n",
    "        land_meta[str(y)] = meta\n",
    "    land = pd.concat(parts, ignore_index=True)\n",
    "    land.to_csv(CACHE_LAND, index=False)\n",
    "    CACHE_LAND_META.write_text(json.dumps(land_meta, indent=2))\n",
    "    print(\"Wrote cache:\", CACHE_LAND)\n",
    "\n",
    "manifest[\"files_used\"][\"landscape\"] = land_meta\n",
    "\n",
    "print(land.head())\n",
    "print(\"Rows:\", land.shape[0], \"| Years:\", sorted(pd.Series(land[\"year\"]).dropna().unique().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c9d2aac-21c5-4f31-b24c-4c73ef9a794c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: /home/rpat638/econ470/a0/work/hwk3/data/cache/hw3_contract_ratings_2010_2015.csv\n",
      "Deleted: /home/rpat638/econ470/a0/work/hwk3/data/cache/hw3_star_file_manifest.json\n",
      "Loading star ratings for year: 2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n",
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading star ratings for year: 2011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n",
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading star ratings for year: 2012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n",
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading star ratings for year: 2013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n",
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading star ratings for year: 2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n",
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading star ratings for year: 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n",
      "/tmp/ipykernel_3154583/3085827755.py:45: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  s = s.replace({\"\": np.nan, \"NAN\": np.nan, \"NONE\": np.nan, \"NULL\": np.nan})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote cache: /home/rpat638/econ470/a0/work/hwk3/data/cache/hw3_contract_ratings_2010_2015.csv\n",
      "Empty DataFrame\n",
      "Columns: [year, contractid, star_rating, raw_rating]\n",
      "Index: []\n",
      "Rows: 0\n",
      "Star missing rate: nan\n",
      "Raw missing rate: nan\n"
     ]
    }
   ],
   "source": [
    "STAR_RE = re.compile(r\"^[A-Z][0-9]{4}$\")\n",
    "\n",
    "def _star_year_candidates(year: int) -> list[Path]:\n",
    "    y = str(year)\n",
    "    candidates = []\n",
    "\n",
    "    # common folder structure\n",
    "    for folder in [\n",
    "        STAR_DIR / \"Extracted Star Ratings\" / y,\n",
    "        STAR_DIR / \"Extracted Star Ratings\" / y / \"summary\",\n",
    "        STAR_DIR / \"Extracted Star Ratings\",\n",
    "    ]:\n",
    "        if folder.exists():\n",
    "            candidates += safe_rglob(folder, \"*.csv\", limit=200000)\n",
    "\n",
    "    if not candidates:\n",
    "        candidates = [p for p in safe_rglob(STAR_DIR, \"*.csv\", limit=200000) if y in str(p)]\n",
    "\n",
    "    # de-dup\n",
    "    return sorted(set(candidates))\n",
    "\n",
    "def _best_contract_col(df: pd.DataFrame) -> tuple[str | None, float]:\n",
    "    cols = list(df.columns)\n",
    "    contract_like = [c for c in cols if \"contract\" in c]\n",
    "    best_c = None\n",
    "    best_score = -1.0\n",
    "    for c in contract_like:\n",
    "        v = clean_contract_id(df[c])\n",
    "        cov = float(v.notna().mean())\n",
    "        m = float(v.dropna().str.match(STAR_RE).mean()) if cov > 0 else 0.0\n",
    "        score = 0.7 * m + 0.3 * cov\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_c = c\n",
    "    return best_c, best_score\n",
    "\n",
    "def _best_star_col(df: pd.DataFrame) -> str | None:\n",
    "    cols = list(df.columns)\n",
    "    for cand in [\"partc_score\", \"star_rating\", \"star\", \"overall_star\", \"overall_rating\"]:\n",
    "        if cand in cols:\n",
    "            return cand\n",
    "\n",
    "    best = None\n",
    "    best_score = -10**9\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if any(t in cl for t in [\"contract\", \"org\", \"state\", \"county\", \"fips\", \"name\", \"ssa\"]):\n",
    "            continue\n",
    "        x = to_num_clean(df[c])\n",
    "        if x.notna().mean() < 0.25:\n",
    "            continue\n",
    "        uniq = x.nunique(dropna=True)\n",
    "        if uniq < 2 or uniq > 14:\n",
    "            continue\n",
    "        vals = sorted(set(x.dropna().unique().tolist()))\n",
    "        half_ok = all(abs(v * 2 - round(v * 2)) < 1e-6 for v in vals[: min(10, len(vals))])\n",
    "        in_range = ((x.dropna() >= 0) & (x.dropna() <= 5.5)).mean() if x.notna().any() else 0.0\n",
    "        score = 0\n",
    "        score += 30 if half_ok else 0\n",
    "        score += 20 if in_range >= 0.85 else 0\n",
    "        score += 10 if (\"star\" in cl or \"rating\" in cl) else 0\n",
    "        score += uniq\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = c\n",
    "    return best\n",
    "\n",
    "def _best_raw_col(df: pd.DataFrame) -> str | None:\n",
    "    cols = list(df.columns)\n",
    "    best = None\n",
    "    best_score = -10**9\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if any(t in cl for t in [\"contract\", \"org\", \"state\", \"county\", \"fips\", \"name\", \"ssa\"]):\n",
    "            continue\n",
    "        if \"star\" in cl or \"rating\" in cl:\n",
    "            continue\n",
    "        x = to_num_clean(df[c])\n",
    "        cov = x.notna().mean()\n",
    "        uniq = x.nunique(dropna=True)\n",
    "        if cov < 0.25 or uniq < 10:\n",
    "            continue\n",
    "        x2 = x.dropna()\n",
    "        in_range = ((x2 >= 0) & (x2 <= 5.5)).mean() if len(x2) else 0.0\n",
    "        score = uniq\n",
    "        if \"raw\" in cl:\n",
    "            score += 600\n",
    "        if \"summary\" in cl:\n",
    "            score += 250\n",
    "        if \"score\" in cl:\n",
    "            score += 150\n",
    "        if in_range >= 0.85:\n",
    "            score += 400\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best = c\n",
    "    return best\n",
    "\n",
    "def _measure_cols_for_mean(df: pd.DataFrame) -> list[str]:\n",
    "    cols = list(df.columns)\n",
    "    keep = []\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if any(t in cl for t in [\"contract\", \"org\", \"state\", \"county\", \"fips\", \"name\", \"ssa\"]):\n",
    "            continue\n",
    "        if \"star\" in cl or \"rating\" in cl:\n",
    "            continue\n",
    "        x = to_num_clean(df[c])\n",
    "        cov = x.notna().mean()\n",
    "        uniq = x.nunique(dropna=True)\n",
    "        if cov < 0.25 or uniq < 10:\n",
    "            continue\n",
    "        x2 = x.dropna()\n",
    "        in_range = ((x2 >= 0) & (x2 <= 5.5)).mean() if len(x2) else 0.0\n",
    "        if in_range >= 0.85:\n",
    "            keep.append(c)\n",
    "    return keep\n",
    "\n",
    "def _score_star_file(path: Path) -> tuple[float, dict]:\n",
    "    try:\n",
    "        dfh = read_any(path, nrows_scan=200)\n",
    "        dfh = normalize_columns(dfh)\n",
    "    except Exception:\n",
    "        return -10**9, {\"file\": str(path), \"error\": \"read_fail\"}\n",
    "\n",
    "    c_contract, cscore = _best_contract_col(dfh)\n",
    "    c_star = _best_star_col(dfh)\n",
    "    c_raw = _best_raw_col(dfh)\n",
    "    measures = _measure_cols_for_mean(dfh)\n",
    "\n",
    "    # require a decent contract column\n",
    "    if c_contract is None or cscore < 0.10:\n",
    "        return -10**9, {\"file\": str(path), \"error\": \"no_contract\"}\n",
    "\n",
    "    score = 0.0\n",
    "    score += 2000 * cscore\n",
    "    score += 200 if c_star is not None else 0\n",
    "    score += 300 if c_raw is not None else 0\n",
    "    score += min(300, 10 * len(measures))\n",
    "\n",
    "    meta = {\n",
    "        \"file\": str(path),\n",
    "        \"contract_col\": c_contract,\n",
    "        \"contract_score\": float(cscore),\n",
    "        \"star_col\": c_star,\n",
    "        \"raw_col\": c_raw,\n",
    "        \"n_measures\": int(len(measures)),\n",
    "    }\n",
    "    return score, meta\n",
    "\n",
    "def find_star_file_for_year(year: int) -> tuple[Path, dict]:\n",
    "    cand = _star_year_candidates(year)\n",
    "    if not cand:\n",
    "        raise FileNotFoundError(f\"No star CSV candidates for year {year} under {STAR_DIR}\")\n",
    "\n",
    "    scored = []\n",
    "    for p in cand:\n",
    "        sc, meta = _score_star_file(p)\n",
    "        scored.append((sc, p, meta))\n",
    "    scored.sort(key=lambda t: t[0], reverse=True)\n",
    "\n",
    "    best_sc, best_p, best_meta = scored[0]\n",
    "    if best_sc <= -10**8:\n",
    "        raise RuntimeError(f\"Could not find a usable star file for year {year}. Best score too low.\")\n",
    "\n",
    "    # keep top few for debugging\n",
    "    best_meta[\"top3\"] = [{\"score\": float(scored[i][0]), \"file\": str(scored[i][1])} for i in range(min(3, len(scored)))]\n",
    "    return best_p, best_meta\n",
    "\n",
    "def load_star_year(year: int) -> tuple[pd.DataFrame, dict]:\n",
    "    p, meta_pick = find_star_file_for_year(year)\n",
    "\n",
    "    df = read_any(p, nrows_scan=200)\n",
    "    df = normalize_columns(df)\n",
    "\n",
    "    c_contract, cscore = _best_contract_col(df)\n",
    "    if c_contract is None:\n",
    "        raise KeyError(f\"Chosen star file has no contract column after load. year={year} file={p}\")\n",
    "\n",
    "    c_star = _best_star_col(df)\n",
    "    c_raw = _best_raw_col(df)\n",
    "    measures = _measure_cols_for_mean(df)\n",
    "\n",
    "    out = pd.DataFrame()\n",
    "    out[\"year\"] = int(year)\n",
    "    out[\"contractid\"] = clean_contract_id(df[c_contract])\n",
    "\n",
    "    out[\"star_rating\"] = to_num_clean(df[c_star]) if (c_star is not None and c_star in df.columns) else np.nan\n",
    "\n",
    "    raw_source = None\n",
    "    if c_raw is not None and c_raw in df.columns:\n",
    "        out[\"raw_rating\"] = to_num_clean(df[c_raw])\n",
    "        raw_source = f\"raw_col:{c_raw}\"\n",
    "    else:\n",
    "        if len(measures) >= 5:\n",
    "            out[\"raw_rating\"] = pd.concat([to_num_clean(df[c]) for c in measures], axis=1).mean(axis=1, skipna=True)\n",
    "            raw_source = f\"mean_of_measures:n={len(measures)}\"\n",
    "        else:\n",
    "            out[\"raw_rating\"] = np.nan\n",
    "            raw_source = \"missing_raw\"\n",
    "\n",
    "    out = out.dropna(subset=[\"contractid\"])\n",
    "    out = out.groupby([\"year\", \"contractid\"], as_index=False).agg(\n",
    "        star_rating=(\"star_rating\", \"first\"),\n",
    "        raw_rating=(\"raw_rating\", \"first\"),\n",
    "    )\n",
    "\n",
    "    meta = {\n",
    "        \"file\": str(p),\n",
    "        \"contract_col\": c_contract,\n",
    "        \"contract_score\": float(cscore),\n",
    "        \"star_col\": c_star,\n",
    "        \"raw_source\": raw_source,\n",
    "        \"n_measures\": int(len(measures)),\n",
    "        \"top3\": meta_pick.get(\"top3\", []),\n",
    "    }\n",
    "    return out, meta\n",
    "\n",
    "CACHE_STAR = CACHE_DIR / \"hw3_contract_ratings_2010_2015.csv\"\n",
    "CACHE_STAR_META = CACHE_DIR / \"hw3_star_file_manifest.json\"\n",
    "\n",
    "if FORCE_REBUILD_STARS:\n",
    "    for p in [CACHE_STAR, CACHE_STAR_META]:\n",
    "        if p.exists():\n",
    "            p.unlink()\n",
    "            print(\"Deleted:\", p)\n",
    "\n",
    "if CACHE_STAR.exists():\n",
    "    stars = pd.read_csv(CACHE_STAR, dtype={\"contractid\": str})\n",
    "    star_meta = json.loads(CACHE_STAR_META.read_text()) if CACHE_STAR_META.exists() else {}\n",
    "    print(\"Loaded cached stars:\", CACHE_STAR, \"rows:\", stars.shape[0])\n",
    "else:\n",
    "    parts = []\n",
    "    star_meta = {}\n",
    "    for y in YEARS:\n",
    "        print(\"Loading star ratings for year:\", y)\n",
    "        d, meta = load_star_year(y)\n",
    "        parts.append(d)\n",
    "        star_meta[str(y)] = meta\n",
    "    stars = pd.concat(parts, ignore_index=True)\n",
    "    stars.to_csv(CACHE_STAR, index=False)\n",
    "    CACHE_STAR_META.write_text(json.dumps(star_meta, indent=2))\n",
    "    print(\"Wrote cache:\", CACHE_STAR)\n",
    "\n",
    "manifest[\"files_used\"][\"star_ratings\"] = star_meta\n",
    "\n",
    "print(stars.head())\n",
    "print(\"Rows:\", stars.shape[0])\n",
    "print(\"Star missing rate:\", float(stars[\"star_rating\"].isna().mean()) if len(stars) else np.nan)\n",
    "print(\"Raw missing rate:\", float(stars[\"raw_rating\"].isna().mean()) if len(stars) else np.nan)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (econ470)",
   "language": "python",
   "name": "econ470-a0kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
