{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "660b5ded-e16f-4b01-af8a-131de48a9033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deleted data/cache/hw3_benchmark_2010.csv\n",
      "deleted data/cache/hw3_benchmark_2011.csv\n",
      "deleted data/cache/hw3_contract_ratings_2010.csv\n",
      "deleted data/cache/hw3_contract_ratings_2011.csv\n",
      "deleted data/cache/hw3_penetration_2010.csv\n",
      "deleted data/cache/hw3_penetration_2011.csv\n",
      "deleted data/cache/hw3_plan_county_year_enrollment_2010.csv\n",
      "deleted data/cache/hw3_plan_county_year_enrollment_2011.csv\n",
      "deleted data/cache/hw3_plan_county_year_enrollment_2012.csv\n",
      "deleted data/cache/hw3_service_area_2010.csv\n",
      "deleted data/cache/hw3_service_area_2011.csv\n",
      "deleted data/cache/hw3_service_area_2012.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CACHE_DIR = Path(\"data/cache\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "to_delete = []\n",
    "to_delete += list(CACHE_DIR.glob(\"hw3_contract_ratings_*.csv\"))\n",
    "to_delete += list(CACHE_DIR.glob(\"hw3_benchmark_*.csv\"))\n",
    "to_delete += list(CACHE_DIR.glob(\"hw3_plan_county_year_enrollment_*.csv\"))\n",
    "to_delete += list(CACHE_DIR.glob(\"hw3_penetration_*.csv\"))\n",
    "to_delete += list(CACHE_DIR.glob(\"hw3_service_area_*.csv\"))\n",
    "\n",
    "for p in sorted(set(to_delete)):\n",
    "    try:\n",
    "        p.unlink()\n",
    "        print(\"deleted\", p)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca4bbcf-babd-4fa3-9c39-ceb34ce5e0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MA_ROOT: /scion/5261/econ470001/ma-data/ma\n",
      "FIXMAP: /home/rpat638/econ470/a0/work/hwk3/data/cache/hw3_fixmap.json\n",
      "Outputs: /home/rpat638/econ470/a0/work/hwk3/data/processed\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import re\n",
    "import json\n",
    "import io\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, Iterable\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 240)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "YEARS = list(range(2010, 2016))\n",
    "\n",
    "HWK3_ROOT = Path.cwd()\n",
    "CACHE_DIR = HWK3_ROOT / \"data\" / \"cache\"\n",
    "PROCESSED_DIR = HWK3_ROOT / \"data\" / \"processed\"\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PROCESSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "OUT_PLAN_COUNTY_YEAR = PROCESSED_DIR / \"hw3_plan_county_year_2010_2015.csv\"\n",
    "OUT_PLAN_YEAR        = PROCESSED_DIR / \"hw3_plan_year_2010_2015.csv\"\n",
    "OUT_CONTRACT_RATINGS = PROCESSED_DIR / \"hw3_contract_ratings_2010_2015.csv\"\n",
    "OUT_RATING_DIST      = PROCESSED_DIR / \"hw3_rating_distribution_2010_2015.csv\"\n",
    "\n",
    "FIXMAP_PATH = CACHE_DIR / \"hw3_fixmap.json\"\n",
    "if not FIXMAP_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Missing {FIXMAP_PATH}. Run your probe notebook first.\")\n",
    "\n",
    "FIXMAP = json.loads(FIXMAP_PATH.read_text())\n",
    "\n",
    "CANDIDATE_MA_ROOTS = [\n",
    "    Path(\"/scion/5261/econ470001/ma-data/ma\"),\n",
    "    Path(\"/home/rpat638/econ470/a0/work/ma-data/ma\"),\n",
    "    HWK3_ROOT.parent / \"ma-data\" / \"ma\",\n",
    "]\n",
    "\n",
    "def pick_existing(paths: list[Path]) -> Path:\n",
    "    for p in paths:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"No MA_ROOT found:\\n\" + \"\\n\".join(map(str, paths)))\n",
    "\n",
    "MA_ROOT = pick_existing(CANDIDATE_MA_ROOTS)\n",
    "\n",
    "ENROLL_DIR = MA_ROOT / \"enrollment\" / \"Extracted Data\"\n",
    "SAREA_DIR  = MA_ROOT / \"service-area\" / \"Extracted Data\"\n",
    "PEN_DIR    = MA_ROOT / \"penetration\" / \"Extracted Data\"\n",
    "STARS_DIR  = MA_ROOT / \"star-ratings\" / \"Extracted Star Ratings\"\n",
    "BENCH_DIR  = MA_ROOT / \"benchmarks\"\n",
    "\n",
    "TERRITORIES = {\"VI\", \"PR\", \"MP\", \"GU\", \"AS\"}\n",
    "\n",
    "print(\"MA_ROOT:\", MA_ROOT)\n",
    "print(\"FIXMAP:\", FIXMAP_PATH)\n",
    "print(\"Outputs:\", PROCESSED_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1ed80f-7dd6-454e-8717-4c749cf45fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_columns(cols: Iterable[str]) -> list[str]:\n",
    "    out = []\n",
    "    seen = {}\n",
    "    for c in cols:\n",
    "        c0 = str(c).strip().lower()\n",
    "        c0 = re.sub(r\"[^\\w]+\", \"_\", c0)\n",
    "        c0 = re.sub(r\"_+\", \"_\", c0).strip(\"_\")\n",
    "        if c0 == \"\":\n",
    "            c0 = \"col\"\n",
    "        if c0 in seen:\n",
    "            seen[c0] += 1\n",
    "            c0 = f\"{c0}_{seen[c0]}\"\n",
    "        else:\n",
    "            seen[c0] = 0\n",
    "        out.append(c0)\n",
    "    return out\n",
    "\n",
    "def read_csv_safe(path: Path, **kwargs) -> pd.DataFrame:\n",
    "    engine = kwargs.get(\"engine\", None)\n",
    "    if engine == \"python\":\n",
    "        kwargs.pop(\"low_memory\", None)\n",
    "    else:\n",
    "        kwargs.setdefault(\"low_memory\", False)\n",
    "    kwargs.setdefault(\"on_bad_lines\", \"skip\")\n",
    "\n",
    "    for enc in [\"utf-8\", \"latin-1\", \"cp1252\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc, **kwargs)\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "    return pd.read_csv(path, **kwargs)\n",
    "\n",
    "def digits_only(x) -> str:\n",
    "    return re.sub(r\"\\D\", \"\", \"\" if pd.isna(x) else str(x))\n",
    "\n",
    "def digits_n(x, n: int) -> Optional[str]:\n",
    "    d = digits_only(x)\n",
    "    if d == \"\":\n",
    "        return None\n",
    "    if len(d) > n:\n",
    "        d = d[-n:]\n",
    "    return d.zfill(n)\n",
    "\n",
    "def clean_plan_id(x) -> Optional[str]:\n",
    "    return digits_n(x, 3)\n",
    "\n",
    "def clean_fips(x) -> Optional[str]:\n",
    "    d = digits_n(x, 5)\n",
    "    if d is None:\n",
    "        return None\n",
    "    return d if len(d) == 5 else None\n",
    "\n",
    "def clean_ssa(x) -> Optional[str]:\n",
    "    d = digits_n(x, 5)\n",
    "    if d is None:\n",
    "        return None\n",
    "    return d if len(d) == 5 else None\n",
    "\n",
    "def _as_series(x):\n",
    "    if isinstance(x, pd.DataFrame):\n",
    "        return x.iloc[:, 0]\n",
    "    return x\n",
    "\n",
    "def to_float_series(x) -> pd.Series:\n",
    "    x = _as_series(x)\n",
    "    s = x.astype(str)\n",
    "    s = s.str.replace(\",\", \"\", regex=False)\n",
    "    s = s.str.replace(\"*\", \"\", regex=False)\n",
    "    s = s.str.strip()\n",
    "    num = s.str.extract(r\"(\\d+(?:\\.\\d+)?)\", expand=False)\n",
    "    return pd.to_numeric(num, errors=\"coerce\")\n",
    "\n",
    "def clean_contractid_series(x: pd.Series) -> pd.Series:\n",
    "    s = _as_series(x).astype(str).str.strip()\n",
    "    s = s.str.extract(r\"([A-Z]\\d{4,5})\", expand=False)\n",
    "    return s\n",
    "\n",
    "def dedupe_cols(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    if df.columns.duplicated().any():\n",
    "        df = df.loc[:, ~df.columns.duplicated()].copy()\n",
    "    return df\n",
    "\n",
    "def standardize_contractid(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = dedupe_cols(df)\n",
    "    cols = list(df.columns)\n",
    "\n",
    "    contract_cols = [c for c in cols if \"contract\" in c]\n",
    "    base = contract_cols[0] if contract_cols else cols[0]\n",
    "\n",
    "    contractid = clean_contractid_series(df[base])\n",
    "\n",
    "    drop_cols = list(dict.fromkeys(contract_cols + ([base] if base not in contract_cols else [])))\n",
    "    df2 = df.drop(columns=[c for c in drop_cols if c in df.columns]).copy()\n",
    "\n",
    "    df2.insert(0, \"contractid\", contractid)\n",
    "    df2 = dedupe_cols(df2)\n",
    "    df2 = df2.dropna(subset=[\"contractid\"]).copy()\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc300d56-8001-435b-a112-e5036ef38c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_enrollment_contract_file(year: int) -> Path:\n",
    "    for p in [ENROLL_DIR / f\"CPSC_Contract_Info_{year}_01.csv\", ENROLL_DIR / f\"CPSC_Contract_Info_{year}_1.csv\"]:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    raise FileNotFoundError(f\"Missing contract info for {year}\")\n",
    "\n",
    "def find_enrollment_month_files(year: int) -> list[Path]:\n",
    "    files = sorted(ENROLL_DIR.glob(f\"CPSC_Enrollment_Info_{year}_*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Missing enrollment months for {year}\")\n",
    "    return files\n",
    "\n",
    "def find_service_area_file(year: int) -> Optional[Path]:\n",
    "    for p in [SAREA_DIR / f\"MA_Cnty_SA_{year}_01.csv\", SAREA_DIR / f\"MA_Cnty_SA_{year}01.csv\", SAREA_DIR / f\"MA_Cnty_SA_{year}.csv\"]:\n",
    "        if p.exists():\n",
    "            return p\n",
    "    hits = sorted(SAREA_DIR.glob(f\"*{year}*SA*csv\"))\n",
    "    return hits[0] if hits else None\n",
    "\n",
    "def find_penetration_month_files(year: int) -> list[Path]:\n",
    "    files = sorted(PEN_DIR.glob(f\"State_County_Penetration_MA_{year}_*.csv\"))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"Missing penetration months for {year}\")\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c73b7c98-ba2d-40f4-8769-ba4dfb7c368c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_contract_info(year: int) -> pd.DataFrame:\n",
    "    p = find_enrollment_contract_file(year)\n",
    "    df = read_csv_safe(p, dtype=str)\n",
    "    df.columns = normalize_columns(df.columns)\n",
    "    df = dedupe_cols(df)\n",
    "\n",
    "    rename_map = {\n",
    "        \"contract_id\": \"contractid\",\n",
    "        \"contract_number\": \"contractid\",\n",
    "        \"plan_id\": \"planid\",\n",
    "        \"organization_type\": \"org_type\",\n",
    "        \"plan_type\": \"plan_type\",\n",
    "        \"offers_part_d\": \"partd\",\n",
    "        \"snp_plan\": \"snp\",\n",
    "        \"eghp\": \"eghp\",\n",
    "        \"organization_name\": \"org_name\",\n",
    "        \"organization_marketing_name\": \"org_marketing_name\",\n",
    "        \"plan_name\": \"plan_name\",\n",
    "        \"parent_organization\": \"parent_org\",\n",
    "    }\n",
    "    for k, v in rename_map.items():\n",
    "        if k in df.columns:\n",
    "            df = df.rename(columns={k: v})\n",
    "\n",
    "    df[\"contractid\"] = df[\"contractid\"].astype(str).str.strip()\n",
    "    df[\"planid\"] = df[\"planid\"].apply(clean_plan_id)\n",
    "\n",
    "    for c in [\"snp\", \"partd\", \"plan_type\", \"org_type\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].astype(str).str.strip()\n",
    "\n",
    "    df = df.dropna(subset=[\"contractid\", \"planid\"])\n",
    "\n",
    "    if \"snp\" in df.columns:\n",
    "        df = df[df[\"snp\"].str.lower().isin([\"no\", \"n\", \"0\", \"false\"])]\n",
    "\n",
    "    planid_num = pd.to_numeric(df[\"planid\"], errors=\"coerce\")\n",
    "    df = df[~(planid_num.between(800, 899, inclusive=\"both\"))]\n",
    "\n",
    "    if \"plan_type\" in df.columns:\n",
    "        df = df[~df[\"plan_type\"].astype(str).str.upper().str.contains(\"PDP\", na=False)]\n",
    "\n",
    "    keep = [c for c in [\n",
    "        \"contractid\",\"planid\",\"org_type\",\"plan_type\",\"partd\",\"eghp\",\n",
    "        \"org_name\",\"org_marketing_name\",\"plan_name\",\"parent_org\"\n",
    "    ] if c in df.columns]\n",
    "    return df[keep].drop_duplicates()\n",
    "\n",
    "def build_plan_county_year_enrollment(year: int) -> pd.DataFrame:\n",
    "    cache_path = CACHE_DIR / f\"hw3_plan_county_year_enrollment_{year}.csv\"\n",
    "    if cache_path.exists():\n",
    "        return pd.read_csv(cache_path, dtype=str)\n",
    "\n",
    "    contract = load_contract_info(year)\n",
    "    frames = []\n",
    "\n",
    "    for p in find_enrollment_month_files(year):\n",
    "        m = int(re.search(rf\"{year}_(\\d\\d)\\.csv$\", p.name).group(1))\n",
    "        dfm = read_csv_safe(p, dtype=str)\n",
    "        dfm.columns = normalize_columns(dfm.columns)\n",
    "        dfm = dedupe_cols(dfm)\n",
    "\n",
    "        if \"contract_number\" in dfm.columns and \"contractid\" not in dfm.columns:\n",
    "            dfm = dfm.rename(columns={\"contract_number\": \"contractid\"})\n",
    "        if \"contract_id\" in dfm.columns and \"contractid\" not in dfm.columns:\n",
    "            dfm = dfm.rename(columns={\"contract_id\": \"contractid\"})\n",
    "        if \"plan_id\" in dfm.columns and \"planid\" not in dfm.columns:\n",
    "            dfm = dfm.rename(columns={\"plan_id\": \"planid\"})\n",
    "        if \"fips_state_county_code\" in dfm.columns and \"fips\" not in dfm.columns:\n",
    "            dfm = dfm.rename(columns={\"fips_state_county_code\": \"fips\"})\n",
    "\n",
    "        keep = [c for c in [\"contractid\",\"planid\",\"fips\",\"state\",\"county\",\"enrollment\"] if c in dfm.columns]\n",
    "        dfm = dfm[keep].copy()\n",
    "\n",
    "        dfm[\"contractid\"] = dfm[\"contractid\"].astype(str).str.strip()\n",
    "        dfm[\"planid\"] = dfm[\"planid\"].apply(clean_plan_id)\n",
    "        dfm[\"fips\"] = dfm[\"fips\"].apply(clean_fips)\n",
    "        dfm[\"enrollment\"] = pd.to_numeric(dfm[\"enrollment\"].astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "        dfm[\"state\"] = dfm.get(\"state\", None)\n",
    "        dfm[\"county\"] = dfm.get(\"county\", None)\n",
    "        if \"state\" in dfm.columns:\n",
    "            dfm[\"state\"] = dfm[\"state\"].astype(str).str.strip()\n",
    "        if \"county\" in dfm.columns:\n",
    "            dfm[\"county\"] = dfm[\"county\"].astype(str).str.strip()\n",
    "\n",
    "        dfm[\"month\"] = m\n",
    "        dfm = dfm.merge(contract[[\"contractid\",\"planid\"]], on=[\"contractid\",\"planid\"], how=\"inner\")\n",
    "        frames.append(dfm)\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    df = df.dropna(subset=[\"fips\",\"contractid\",\"planid\"]).copy()\n",
    "    df = df.sort_values([\"state\",\"county\",\"contractid\",\"planid\",\"month\"])\n",
    "    df[\"fips\"] = df.groupby([\"state\",\"county\"])[\"fips\"].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "    out = (\n",
    "        df.groupby([\"contractid\",\"planid\",\"fips\"], as_index=False)\n",
    "          .agg(avg_enrollment=(\"enrollment\",\"mean\"))\n",
    "    )\n",
    "    dec = (\n",
    "        df[df[\"month\"] == 12]\n",
    "        .groupby([\"contractid\",\"planid\",\"fips\"], as_index=False)\n",
    "        .agg(dec_enrollment=(\"enrollment\",\"sum\"))\n",
    "    )\n",
    "    out = out.merge(dec, on=[\"contractid\",\"planid\",\"fips\"], how=\"left\")\n",
    "\n",
    "    loc = (\n",
    "        df.dropna(subset=[\"state\",\"county\"])\n",
    "          .sort_values([\"contractid\",\"planid\",\"fips\",\"month\"])\n",
    "          .groupby([\"contractid\",\"planid\",\"fips\"], as_index=False)\n",
    "          .tail(1)[[\"contractid\",\"planid\",\"fips\",\"state\",\"county\"]]\n",
    "    )\n",
    "    out = out.merge(loc, on=[\"contractid\",\"planid\",\"fips\"], how=\"left\")\n",
    "\n",
    "    out[\"year\"] = year\n",
    "    out = out.merge(contract, on=[\"contractid\",\"planid\"], how=\"left\")\n",
    "\n",
    "    out.to_csv(cache_path, index=False)\n",
    "    return out\n",
    "\n",
    "def load_service_area(year: int) -> pd.DataFrame:\n",
    "    cache_path = CACHE_DIR / f\"hw3_service_area_{year}.csv\"\n",
    "    if cache_path.exists():\n",
    "        return pd.read_csv(cache_path, dtype=str)\n",
    "\n",
    "    p = find_service_area_file(year)\n",
    "    if p is None:\n",
    "        return pd.DataFrame(columns=[\"contractid\",\"fips\"])\n",
    "\n",
    "    df = read_csv_safe(p, dtype=str)\n",
    "    df.columns = normalize_columns(df.columns)\n",
    "    df = dedupe_cols(df)\n",
    "\n",
    "    if \"contract_id\" in df.columns and \"contractid\" not in df.columns:\n",
    "        df = df.rename(columns={\"contract_id\": \"contractid\"})\n",
    "    if \"contract\" in df.columns and \"contractid\" not in df.columns:\n",
    "        df = df.rename(columns={\"contract\": \"contractid\"})\n",
    "    if \"fips_state_county_code\" in df.columns and \"fips\" not in df.columns:\n",
    "        df = df.rename(columns={\"fips_state_county_code\": \"fips\"})\n",
    "\n",
    "    df[\"contractid\"] = df[\"contractid\"].astype(str).str.strip()\n",
    "    df[\"fips\"] = df[\"fips\"].apply(clean_fips)\n",
    "    df = df.dropna(subset=[\"contractid\",\"fips\"]).drop_duplicates()\n",
    "\n",
    "    out = df[[\"contractid\",\"fips\"]].copy()\n",
    "    out.to_csv(cache_path, index=False)\n",
    "    return out\n",
    "\n",
    "def load_penetration(year: int) -> pd.DataFrame:\n",
    "    cache_path = CACHE_DIR / f\"hw3_penetration_{year}.csv\"\n",
    "    if cache_path.exists():\n",
    "        return pd.read_csv(cache_path, dtype=str)\n",
    "\n",
    "    frames = []\n",
    "    for p in find_penetration_month_files(year):\n",
    "        m = int(re.search(rf\"{year}_(\\d\\d)\\.csv$\", p.name).group(1))\n",
    "        dfm = read_csv_safe(p, dtype=str)\n",
    "        dfm.columns = normalize_columns(dfm.columns)\n",
    "        dfm = dedupe_cols(dfm)\n",
    "\n",
    "        if \"state_name\" in dfm.columns:\n",
    "            dfm = dfm.rename(columns={\"state_name\": \"state\"})\n",
    "        if \"county_name\" in dfm.columns:\n",
    "            dfm = dfm.rename(columns={\"county_name\": \"county\"})\n",
    "\n",
    "        keep = [c for c in [\"state\",\"county\",\"fips\",\"ssa\",\"eligibles\",\"enrolled\"] if c in dfm.columns]\n",
    "        dfm = dfm[keep].copy()\n",
    "\n",
    "        dfm[\"fips\"] = dfm[\"fips\"].apply(clean_fips)\n",
    "        dfm[\"ssa\"] = dfm[\"ssa\"].apply(clean_ssa)\n",
    "        dfm[\"eligibles\"] = pd.to_numeric(dfm[\"eligibles\"].astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "        dfm[\"enrolled\"]  = pd.to_numeric(dfm[\"enrolled\"].astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "        dfm[\"month\"] = m\n",
    "        frames.append(dfm)\n",
    "\n",
    "    df = pd.concat(frames, ignore_index=True)\n",
    "    df = df.dropna(subset=[\"fips\"]).copy()\n",
    "    df = df.sort_values([\"state\",\"county\",\"month\"])\n",
    "    df[\"fips\"] = df.groupby([\"state\",\"county\"])[\"fips\"].transform(lambda x: x.ffill().bfill())\n",
    "\n",
    "    out = (\n",
    "        df.groupby([\"fips\"], as_index=False)\n",
    "          .agg(avg_eligibles=(\"eligibles\",\"mean\"),\n",
    "               avg_enrolled=(\"enrolled\",\"mean\"),\n",
    "               ssa=(\"ssa\",\"last\"))\n",
    "    )\n",
    "    out[\"year\"] = year\n",
    "    out.to_csv(cache_path, index=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fbde2b74-5c31-417b-a835-ae0972e2088b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_star_csv(path: Path, skiprows: int) -> pd.DataFrame:\n",
    "    df = read_csv_safe(path, skiprows=skiprows, header=0, dtype=str)\n",
    "    if df.shape[1] == 1:\n",
    "        df = read_csv_safe(path, skiprows=skiprows, header=0, dtype=str, sep=None, engine=\"python\")\n",
    "    df.columns = normalize_columns(df.columns)\n",
    "    df = dedupe_cols(df)\n",
    "    df = standardize_contractid(df)\n",
    "    return df\n",
    "\n",
    "def _domain_raw_score(domain: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [c for c in domain.columns if c != \"contractid\"]\n",
    "    keep = []\n",
    "    skip_tokens = [\n",
    "        \"contract\",\"organization\",\"org\",\"parent\",\"marketing\",\"name\",\"type\",\n",
    "        \"overall\",\"summary\",\"domain\",\"icon\",\"performer\",\"sanction\",\"snp\",\n",
    "    ]\n",
    "    for c in cols:\n",
    "        cl = c.lower()\n",
    "        if any(t in cl for t in skip_tokens):\n",
    "            continue\n",
    "        v = to_float_series(domain[c])\n",
    "        n = int(v.notna().sum())\n",
    "        if n < 150:\n",
    "            continue\n",
    "        frac = float(((v >= 1.0) & (v <= 5.0)).mean())\n",
    "        if frac < 0.75:\n",
    "            continue\n",
    "        keep.append(c)\n",
    "\n",
    "    if not keep:\n",
    "        return domain[[\"contractid\"]].drop_duplicates().assign(raw_score=np.nan)\n",
    "\n",
    "    m = domain[[\"contractid\"] + keep].copy()\n",
    "    for c in keep:\n",
    "        m[c] = to_float_series(m[c])\n",
    "    m[\"raw_score\"] = m[keep].mean(axis=1, skipna=True)\n",
    "    return m[[\"contractid\",\"raw_score\"]].drop_duplicates()\n",
    "\n",
    "def load_star_ratings(year: int) -> pd.DataFrame:\n",
    "    cache_path = CACHE_DIR / f\"hw3_contract_ratings_{year}.csv\"\n",
    "    if cache_path.exists():\n",
    "        dfc = pd.read_csv(cache_path, dtype=str)\n",
    "        need = {\"contractid\",\"year\",\"partc_rating\",\"raw_score\",\"new_contract\"}\n",
    "        if need.issubset(set(dfc.columns)):\n",
    "            return dfc\n",
    "\n",
    "    yk = str(year)\n",
    "    sfix = FIXMAP[\"stars\"][yk]\n",
    "\n",
    "    dom_path = Path(sfix[\"domain_path\"])\n",
    "    sum_path = Path(sfix[\"summary_path\"])\n",
    "\n",
    "    skip_dom = {2010:3, 2011:1, 2012:1, 2013:1, 2014:1, 2015:1}[year]\n",
    "    skip_sum = {2010:1, 2011:1, 2012:1, 2013:1, 2014:1, 2015:1}[year]\n",
    "\n",
    "    domain = _read_star_csv(dom_path, skiprows=skip_dom)\n",
    "    summary = _read_star_csv(sum_path, skiprows=skip_sum)\n",
    "\n",
    "    partc_col = sfix[\"picked_partc_col\"]\n",
    "    partd_col = sfix.get(\"picked_partd_col\")\n",
    "    partcd_col = sfix.get(\"picked_partcd_col\")\n",
    "\n",
    "    if year == 2015:\n",
    "        cand = [c for c in summary.columns if \"part_c\" in c and \"summary\" in c and c != \"contractid\"]\n",
    "        if cand:\n",
    "            partc_col = cand[0]\n",
    "\n",
    "    def safe_col(colname: Optional[str]) -> Optional[str]:\n",
    "        if colname is None:\n",
    "            return None\n",
    "        return colname if colname in summary.columns else None\n",
    "\n",
    "    partc_col = safe_col(partc_col)\n",
    "    partd_col = safe_col(partd_col)\n",
    "    partcd_col = safe_col(partcd_col)\n",
    "\n",
    "    out = summary[[\"contractid\"]].drop_duplicates().copy()\n",
    "    out[\"year\"] = year\n",
    "\n",
    "    out[\"partc_rating\"] = np.nan\n",
    "    out[\"partd_rating\"] = np.nan\n",
    "    out[\"partcd_rating\"] = np.nan\n",
    "    out[\"new_contract\"] = 0\n",
    "\n",
    "    def flag_new(txt: pd.Series) -> pd.Series:\n",
    "        t = txt.astype(str).str.lower()\n",
    "        return t.str.contains(\"too new|new to rate|plan too new\", regex=True, na=False)\n",
    "\n",
    "    if partc_col is not None:\n",
    "        tmp = summary[[\"contractid\", partc_col]].copy()\n",
    "        tmp[\"partc_rating\"] = to_float_series(tmp[partc_col])\n",
    "        out = out.merge(tmp[[\"contractid\",\"partc_rating\"]], on=\"contractid\", how=\"left\")\n",
    "\n",
    "        newf = summary[[\"contractid\"]].assign(_new=flag_new(summary[partc_col]).values)\n",
    "        newf = newf.groupby(\"contractid\")[\"_new\"].max().reset_index()\n",
    "        out = out.merge(newf, on=\"contractid\", how=\"left\")\n",
    "        out[\"new_contract\"] = np.where(out[\"_new\"].fillna(False), 1, out[\"new_contract\"])\n",
    "        out = out.drop(columns=[\"_new\"])\n",
    "\n",
    "    if partd_col is not None:\n",
    "        tmp = summary[[\"contractid\", partd_col]].copy()\n",
    "        tmp[\"partd_rating\"] = to_float_series(tmp[partd_col])\n",
    "        out = out.merge(tmp[[\"contractid\",\"partd_rating\"]], on=\"contractid\", how=\"left\")\n",
    "\n",
    "    if partcd_col is not None:\n",
    "        tmp = summary[[\"contractid\", partcd_col]].copy()\n",
    "        tmp[\"partcd_rating\"] = to_float_series(tmp[partcd_col])\n",
    "        out = out.merge(tmp[[\"contractid\",\"partcd_rating\"]], on=\"contractid\", how=\"left\")\n",
    "\n",
    "    raw = _domain_raw_score(domain)\n",
    "    out = out.merge(raw, on=\"contractid\", how=\"left\")\n",
    "\n",
    "    out.loc[out[\"new_contract\"] == 1, [\"partc_rating\",\"partd_rating\",\"partcd_rating\",\"raw_score\"]] = np.nan\n",
    "\n",
    "    n_star = int(pd.to_numeric(out[\"partc_rating\"], errors=\"coerce\").notna().sum())\n",
    "    n_raw  = int(pd.to_numeric(out[\"raw_score\"], errors=\"coerce\").notna().sum())\n",
    "    print(f\"stars {year}: contracts={out.shape[0]} partc_nonmiss={n_star} raw_nonmiss={n_raw}\")\n",
    "\n",
    "    out.to_csv(cache_path, index=False)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81491d7d-fa95-405b-8060-ae5a6173b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_benchmark(year: int) -> pd.DataFrame:\n",
    "    cache_path = CACHE_DIR / f\"hw3_benchmark_{year}.csv\"\n",
    "    if cache_path.exists():\n",
    "        dfc = pd.read_csv(cache_path, dtype=str)\n",
    "        if {\"ssa\",\"year\"}.issubset(set(dfc.columns)):\n",
    "            return dfc\n",
    "\n",
    "    yk = str(year)\n",
    "    b = FIXMAP[\"benchmarks\"][yk]\n",
    "    if b.get(\"benchmark_path\") is None or b.get(\"sep\") is None or b.get(\"start_line\") is None:\n",
    "        out = pd.DataFrame({\"ssa\": [], \"risk_ab\": [], \"year\": []})\n",
    "        out.to_csv(cache_path, index=False)\n",
    "        return out\n",
    "\n",
    "    path = Path(b[\"benchmark_path\"])\n",
    "    sep = b[\"sep\"]\n",
    "    start = int(b[\"start_line\"])\n",
    "\n",
    "    lines = path.read_text(errors=\"ignore\").splitlines()\n",
    "    data = \"\\n\".join(lines[start:])\n",
    "\n",
    "    if sep == \"whitespace\":\n",
    "        df = pd.read_csv(io.StringIO(data), sep=r\"\\s+\", header=None, dtype=str, engine=\"python\", on_bad_lines=\"skip\")\n",
    "    else:\n",
    "        df = pd.read_csv(io.StringIO(data), sep=sep, header=None, dtype=str, engine=\"python\", on_bad_lines=\"skip\")\n",
    "\n",
    "    if df.shape[1] < 2:\n",
    "        out = pd.DataFrame({\"ssa\": [], \"risk_ab\": [], \"year\": []})\n",
    "        out.to_csv(cache_path, index=False)\n",
    "        return out\n",
    "\n",
    "    df0 = df.iloc[:, 0].astype(str).str.strip()\n",
    "    ssa = df0.apply(clean_ssa)\n",
    "\n",
    "    risk_ab = pd.Series(np.nan, index=df.index, dtype=\"float64\")\n",
    "    if df.shape[1] >= 9:\n",
    "        risk_ab = pd.to_numeric(df.iloc[:, 8].astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "    else:\n",
    "        \n",
    "        cands = []\n",
    "        for j in range(1, df.shape[1]):\n",
    "            v = pd.to_numeric(df.iloc[:, j].astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "            if int(v.notna().sum()) < 1000:\n",
    "                continue\n",
    "            med = float(v.median())\n",
    "            if 200 <= med <= 2000:\n",
    "                cands.append((j, med))\n",
    "        if cands:\n",
    "            j = sorted(cands, key=lambda t: (t[0], t[1]))[-1][0]\n",
    "            risk_ab = pd.to_numeric(df.iloc[:, j].astype(str).str.replace(\",\", \"\", regex=False), errors=\"coerce\")\n",
    "\n",
    "    out = pd.DataFrame({\"ssa\": ssa, \"risk_ab\": risk_ab, \"year\": year})\n",
    "    out = out.dropna(subset=[\"ssa\"]).drop_duplicates(subset=[\"ssa\"])\n",
    "    out.to_csv(cache_path, index=False)\n",
    "    print(\"benchmark\", year, \"rows\", out.shape[0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "664f1606-a546-437e-b71e-12cce039ee3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "YEAR 2010\n",
      "stars 2010: contracts=708 partc_nonmiss=0 raw_nonmiss=0\n",
      "benchmark 2010 rows 3247\n",
      "enrollment rows 675013\n",
      "service area pairs 436236\n",
      "penetration counties 3280\n",
      "contract ratings rows 708\n",
      "benchmark counties 3247\n",
      "final rows 109944\n",
      "Star_Rating non-missing 0\n",
      "raw_score non-missing 0\n",
      "mkt_share non-missing 86786\n",
      "\n",
      "======================================================================\n",
      "YEAR 2011\n",
      "stars 2011: contracts=575 partc_nonmiss=0 raw_nonmiss=507\n",
      "benchmark 2011 rows 2161\n",
      "enrollment rows 507548\n",
      "service area pairs 381465\n",
      "penetration counties 3228\n",
      "contract ratings rows 575\n",
      "benchmark counties 2161\n",
      "final rows 67967\n",
      "Star_Rating non-missing 0\n",
      "raw_score non-missing 57318\n",
      "mkt_share non-missing 54352\n",
      "\n",
      "======================================================================\n",
      "YEAR 2012\n",
      "stars 2012: contracts=569 partc_nonmiss=0 raw_nonmiss=483\n",
      "benchmark 2012 rows 3246\n",
      "enrollment rows 498019\n",
      "service area pairs 374396\n",
      "penetration counties 3224\n",
      "contract ratings rows 569\n",
      "benchmark counties 3246\n",
      "final rows 67206\n",
      "Star_Rating non-missing 0\n",
      "raw_score non-missing 59512\n",
      "mkt_share non-missing 52887\n",
      "\n",
      "======================================================================\n",
      "YEAR 2013\n",
      "stars 2013: contracts=578 partc_nonmiss=0 raw_nonmiss=472\n",
      "benchmark 2013 rows 3246\n",
      "enrollment rows 488390\n",
      "service area pairs 376990\n",
      "penetration counties 3224\n",
      "contract ratings rows 578\n",
      "benchmark counties 3246\n",
      "final rows 67739\n",
      "Star_Rating non-missing 0\n",
      "raw_score non-missing 65076\n",
      "mkt_share non-missing 53406\n",
      "\n",
      "======================================================================\n",
      "YEAR 2014\n",
      "stars 2014: contracts=677 partc_nonmiss=0 raw_nonmiss=0\n",
      "benchmark 2014 rows 3246\n",
      "enrollment rows 506498\n",
      "service area pairs 397494\n",
      "penetration counties 3224\n",
      "contract ratings rows 677\n",
      "benchmark counties 3246\n",
      "final rows 62211\n",
      "Star_Rating non-missing 0\n",
      "raw_score non-missing 0\n",
      "mkt_share non-missing 46567\n",
      "\n",
      "======================================================================\n",
      "YEAR 2015\n",
      "stars 2015: contracts=691 partc_nonmiss=0 raw_nonmiss=0\n",
      "benchmark 2015 rows 3248\n",
      "enrollment rows 470935\n",
      "service area pairs 378211\n",
      "penetration counties 3224\n",
      "contract ratings rows 691\n",
      "benchmark counties 3248\n",
      "final rows 65403\n",
      "Star_Rating non-missing 0\n",
      "raw_score non-missing 0\n",
      "mkt_share non-missing 48544\n",
      "\n",
      "Saved\n",
      "plan_county_year: /home/rpat638/econ470/a0/work/hwk3/data/processed/hw3_plan_county_year_2010_2015.csv (440470, 23)\n",
      "plan_year: /home/rpat638/econ470/a0/work/hwk3/data/processed/hw3_plan_year_2010_2015.csv (13510, 9)\n",
      "contract_ratings: /home/rpat638/econ470/a0/work/hwk3/data/processed/hw3_contract_ratings_2010_2015.csv (3798, 11)\n",
      "rating_dist: /home/rpat638/econ470/a0/work/hwk3/data/processed/hw3_rating_distribution_2010_2015.csv (0, 3)\n"
     ]
    }
   ],
   "source": [
    "all_rows = []\n",
    "all_contract = []\n",
    "\n",
    "for year in YEARS:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"YEAR\", year)\n",
    "\n",
    "    plan = build_plan_county_year_enrollment(year)\n",
    "    sa   = load_service_area(year)\n",
    "    pen  = load_penetration(year)\n",
    "    stars = load_star_ratings(year)\n",
    "    bench = load_benchmark(year)\n",
    "\n",
    "    print(\"enrollment rows\", plan.shape[0])\n",
    "    print(\"service area pairs\", sa.shape[0])\n",
    "    print(\"penetration counties\", pen.shape[0])\n",
    "    print(\"contract ratings rows\", stars.shape[0])\n",
    "    print(\"benchmark counties\", bench.shape[0])\n",
    "\n",
    "    df = plan.merge(sa.drop_duplicates(), on=[\"contractid\",\"fips\"], how=\"inner\")\n",
    "    if \"state\" in df.columns:\n",
    "        df = df[~df[\"state\"].isin(TERRITORIES)].copy()\n",
    "\n",
    "    df = df.merge(pen[[\"fips\",\"avg_eligibles\",\"avg_enrolled\",\"ssa\"]], on=\"fips\", how=\"left\")\n",
    "\n",
    "    df = df.merge(\n",
    "        stars[[\"contractid\",\"partc_rating\",\"partd_rating\",\"partcd_rating\",\"raw_score\",\"new_contract\"]],\n",
    "        on=\"contractid\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    \n",
    "    partd_no = df.get(\"partd\", \"\").astype(str).str.strip().str.lower().isin([\"no\",\"n\",\"0\",\"false\"])\n",
    "    has_partcd = pd.to_numeric(df[\"partcd_rating\"], errors=\"coerce\").notna()\n",
    "\n",
    "    df[\"Star_Rating\"] = np.where(\n",
    "        partd_no,\n",
    "        pd.to_numeric(df[\"partc_rating\"], errors=\"coerce\"),\n",
    "        np.where(has_partcd, pd.to_numeric(df[\"partcd_rating\"], errors=\"coerce\"), pd.to_numeric(df[\"partc_rating\"], errors=\"coerce\"))\n",
    "    )\n",
    "\n",
    "    df = df.merge(bench[[\"ssa\",\"risk_ab\"]], on=\"ssa\", how=\"left\")\n",
    "\n",
    "    df[\"avg_enrollment\"] = pd.to_numeric(df[\"avg_enrollment\"], errors=\"coerce\")\n",
    "    df[\"dec_enrollment\"] = pd.to_numeric(df.get(\"dec_enrollment\", np.nan), errors=\"coerce\")\n",
    "    df[\"enrollment_used\"] = df[\"dec_enrollment\"].where(df[\"dec_enrollment\"].notna(), df[\"avg_enrollment\"])\n",
    "\n",
    "    df[\"avg_enrolled\"] = pd.to_numeric(df[\"avg_enrolled\"], errors=\"coerce\")\n",
    "    df[\"mkt_share\"] = df[\"enrollment_used\"] / df[\"avg_enrolled\"]\n",
    "    df.loc[~np.isfinite(df[\"mkt_share\"]), \"mkt_share\"] = np.nan\n",
    "\n",
    "    df[\"hmo\"] = df.get(\"plan_type\", \"\").astype(str).str.upper().str.contains(\"HMO\").astype(int)\n",
    "    df[\"partd_ind\"] = (~partd_no).astype(int)\n",
    "    df[\"plan_key\"] = df[\"contractid\"].astype(str) + \"-\" + df[\"planid\"].astype(str)\n",
    "    df[\"year\"] = year\n",
    "\n",
    "    keep = [\n",
    "        \"year\",\"contractid\",\"planid\",\"plan_key\",\"fips\",\"state\",\"county\",\n",
    "        \"enrollment_used\",\"avg_enrollment\",\"dec_enrollment\",\"avg_enrolled\",\"avg_eligibles\",\"mkt_share\",\n",
    "        \"Star_Rating\",\"raw_score\",\"new_contract\",\n",
    "        \"partd\",\"partd_ind\",\"plan_type\",\"hmo\",\"org_type\",\n",
    "        \"ssa\",\"risk_ab\",\n",
    "    ]\n",
    "    keep = [c for c in keep if c in df.columns]\n",
    "    df = df[keep].copy()\n",
    "\n",
    "    print(\"final rows\", df.shape[0])\n",
    "    print(\"Star_Rating non-missing\", int(pd.to_numeric(df[\"Star_Rating\"], errors=\"coerce\").notna().sum()))\n",
    "    print(\"raw_score non-missing\", int(pd.to_numeric(df[\"raw_score\"], errors=\"coerce\").notna().sum()))\n",
    "    print(\"mkt_share non-missing\", int(pd.to_numeric(df[\"mkt_share\"], errors=\"coerce\").notna().sum()))\n",
    "\n",
    "    all_rows.append(df)\n",
    "    all_contract.append(stars.assign(year=year))\n",
    "\n",
    "plan_county_year = pd.concat(all_rows, ignore_index=True)\n",
    "contract_ratings = pd.concat(all_contract, ignore_index=True)\n",
    "\n",
    "plan_county_year.to_csv(OUT_PLAN_COUNTY_YEAR, index=False)\n",
    "contract_ratings.to_csv(OUT_CONTRACT_RATINGS, index=False)\n",
    "\n",
    "py = plan_county_year.copy()\n",
    "py[\"enrollment_used\"] = pd.to_numeric(py[\"enrollment_used\"], errors=\"coerce\")\n",
    "plan_year = (\n",
    "    py.groupby([\"year\",\"contractid\",\"planid\"], as_index=False)\n",
    "      .agg(total_enrollment=(\"enrollment_used\",\"sum\"),\n",
    "           Star_Rating=(\"Star_Rating\",\"first\"),\n",
    "           raw_score=(\"raw_score\",\"first\"),\n",
    "           partd=(\"partd\",\"first\"),\n",
    "           hmo=(\"hmo\",\"first\"),\n",
    "           partd_ind=(\"partd_ind\",\"first\"))\n",
    ")\n",
    "plan_year.to_csv(OUT_PLAN_YEAR, index=False)\n",
    "\n",
    "cr = contract_ratings.copy()\n",
    "cr[\"partc_rating\"] = pd.to_numeric(cr[\"partc_rating\"], errors=\"coerce\")\n",
    "dist = (\n",
    "    cr.dropna(subset=[\"partc_rating\"])\n",
    "      .groupby([\"year\",\"partc_rating\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"n_contracts\")\n",
    "      .rename(columns={\"partc_rating\": \"Star_Rating\"})\n",
    "      .sort_values([\"year\",\"Star_Rating\"])\n",
    ")\n",
    "dist.to_csv(OUT_RATING_DIST, index=False)\n",
    "\n",
    "print(\"\\nSaved\")\n",
    "print(\"plan_county_year:\", OUT_PLAN_COUNTY_YEAR, plan_county_year.shape)\n",
    "print(\"plan_year:\", OUT_PLAN_YEAR, plan_year.shape)\n",
    "print(\"contract_ratings:\", OUT_CONTRACT_RATINGS, contract_ratings.shape)\n",
    "print(\"rating_dist:\", OUT_RATING_DIST, dist.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb860b9e-6316-4480-8ff6-4eae181d1f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rows by year\n",
      "year\n",
      "2010    109944\n",
      "2011     67967\n",
      "2012     67206\n",
      "2013     67739\n",
      "2014     62211\n",
      "2015     65403\n",
      "\n",
      "Missing Star_Rating by year\n",
      "year\n",
      "2010    109944\n",
      "2011     67967\n",
      "2012     67206\n",
      "2013     67739\n",
      "2014     62211\n",
      "2015     65403\n",
      "\n",
      "Missing raw_score by year\n",
      "year\n",
      "2010    109944\n",
      "2011     10649\n",
      "2012      7694\n",
      "2013      2663\n",
      "2014     62211\n",
      "2015     65403\n",
      "\n",
      "Star rating distribution (plan-county rows, not unique contracts)\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "2010 checks\n",
      "2010 Star_Rating non-missing 0\n",
      "2010 raw_score non-missing 0\n"
     ]
    }
   ],
   "source": [
    "df = plan_county_year.copy()\n",
    "df[\"Star_Rating\"] = pd.to_numeric(df[\"Star_Rating\"], errors=\"coerce\")\n",
    "df[\"raw_score\"] = pd.to_numeric(df[\"raw_score\"], errors=\"coerce\")\n",
    "df[\"mkt_share\"] = pd.to_numeric(df[\"mkt_share\"], errors=\"coerce\")\n",
    "df[\"enrollment_used\"] = pd.to_numeric(df[\"enrollment_used\"], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nRows by year\")\n",
    "print(df.groupby(\"year\").size().to_string())\n",
    "\n",
    "print(\"\\nMissing Star_Rating by year\")\n",
    "print(df.groupby(\"year\")[\"Star_Rating\"].apply(lambda x: int(x.isna().sum())).to_string())\n",
    "\n",
    "print(\"\\nMissing raw_score by year\")\n",
    "print(df.groupby(\"year\")[\"raw_score\"].apply(lambda x: int(x.isna().sum())).to_string())\n",
    "\n",
    "print(\"\\nStar rating distribution (plan-county rows, not unique contracts)\")\n",
    "print(\n",
    "    df.dropna(subset=[\"Star_Rating\"])\n",
    "      .groupby([\"year\",\"Star_Rating\"])\n",
    "      .size()\n",
    "      .unstack(\"Star_Rating\")\n",
    "      .fillna(0)\n",
    "      .astype(int)\n",
    "      .to_string()\n",
    ")\n",
    "\n",
    "d2010 = df[df[\"year\"] == 2010].copy()\n",
    "print(\"\\n2010 checks\")\n",
    "print(\"2010 Star_Rating non-missing\", int(d2010[\"Star_Rating\"].notna().sum()))\n",
    "print(\"2010 raw_score non-missing\", int(d2010[\"raw_score\"].notna().sum()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (econ470)",
   "language": "python",
   "name": "econ470-a0kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
